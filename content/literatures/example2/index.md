---
title: "CIM-Based Accelerator"
type: landing

# Authors
# If you created a profile for a user (e.g. the default `admin` user), write the username (folder name) here 
# and it will be replaced with their full name and linked to their profile.
#authors:
#- admin
#- Robert Ford

## Author notes (optional)
#author_notes:
#- "Equal contribution"
#- "Equal contribution"

#date: "2013-07-01T00:00:00Z"
doi: ""

## Schedule page publish date (NOT publication's date).
#publishDate: "2017-01-01T00:00:00Z"

## Publication type.
# Legend: 0 = Uncategorized; 1 = Conference paper; 2 = Journal article;
# 3 = Preprint / Working Paper; 4 = Report; 5 = Book; 6 = Book section;
# 7 = Thesis; 8 = Patent
#publication_types: ["2"]

## Publication name and optional abbreviated publication name.
#publication: In *IEEE Transactions on Circuits and Systems I: Regular Papers*
#publication_short: In *ICW*

#abstract: "CIM-DSE: Design Space Exploration for Analyzing and Bridging the Efficiency Gap between the Computing-in-Memory Macro and Accelerator (under review)"

# Summary. An optional shortened abstract.
summary: "While most works focused on CIM designs at the circuit level, there are some interesting works that discussed about CIM from an architecture or system perspective."


#tags: []

# Display this page in the Featured widget?
featured: false


---

### 2022
**[JSSC'22]** Scalable and Programmable Neural Network Inference Accelerator Based on In-Memory Computing



**[JSSC'22]** DIANA: An End-to-End Hybrid Digital and Analog Neural Network SoC for the Edge



### 2023
**[VLSI'23]** A General-Purpose Compute-in-Memory Processor Combining CPU and Deep Learning with Elevated CPU Efficiency and Enhanced Data Locality



**[JSSC'23]** An Energy-Efficient Computing-in-Memory NN Processor With Set-Associate Block-wise Sparsity and Ping-Pong Weight Update



**[ISSCC'23]** 16.7 A 40-310TOPS/W SRAM-Based All-Digital Up to 4b In-Memory Computing Multi-Tiled NN Accelerator in FD-SOI 18nm for Deep-Learning Edge Applications



**[ISSCC'23]** 16.4 TensorCIM: A 28nm 3.7nJ/Gather and 8.3TFLOPS/W FP32 Digital-CIM Tensor Processor for MCM-CIM-Based Beyond-NN Acceleration



**[ISSCC'23]** 16.1 MuITCIM: A 28nm 2.24$\mu$J/Token Attention-Token-Bit Hybrid Sparse Digital CIM-Based Accelerator for Multimodal Transformers